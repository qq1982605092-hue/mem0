"""
ä¸“ä¸šç‰ˆMem0å¿ƒç†å’¨è¯¢å®éªŒ
å€Ÿé‰´ä¸“ä¸šè¯„ä¼°æŒ‡æ ‡ï¼šROUGEã€BLEUã€BERTåˆ†æ•°ã€LLM Judgeç­‰
æ­£ç¡®ç†è§£æ•°æ®ç»“æ„ï¼šæ¯ä¸ªdialog_id = ä¸€ä¸ªç‹¬ç«‹æ‚£è€…
å¢åŠ è¯¦ç»†é—®ç­”è¿‡ç¨‹è®°å½•åŠŸèƒ½
"""

import json
import statistics
from collections import defaultdict
from typing import Dict, List, Union
import nltk
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import pytorch_cos_sim
import numpy as np
from openai import OpenAI
from mem0 import MemoryClient
from tqdm import tqdm
import time
import warnings
from datetime import datetime

# é…ç½®
OPENAI_API_KEY = "sk-21LnRSoxMhJcF51LwUHcQzUsc9KSbTMUgwBD9db1wfyobpQu"
OPENAI_BASE_URL = "https://api.aiclaude.site/v1"
MEM0_API_KEY = "m0-zrcMJP7AjsYZ7jzysLN0VMh3XLiaXWX5Ar6xt5bJ"

# åˆå§‹åŒ–
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
memory = MemoryClient(api_key=MEM0_API_KEY)

# ä¸‹è½½NLTKæ•°æ®
try:
    nltk.download("punkt", quiet=True)
    nltk.download("wordnet", quiet=True)
except:
    pass

# åˆå§‹åŒ–SentenceTransformer
try:
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
except:
    sentence_model = None

warnings.filterwarnings("ignore")

# ===================== ä¸“ä¸šè¯„ä¼°æŒ‡æ ‡ =====================
def calculate_rouge_scores(prediction: str, reference: str) -> Dict[str, float]:
    """è®¡ç®—ROUGEåˆ†æ•°"""
    scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    scores = scorer.score(reference, prediction)
    return {
        "rouge1_f": scores["rouge1"].fmeasure,
        "rouge2_f": scores["rouge2"].fmeasure,
        "rougeL_f": scores["rougeL"].fmeasure,
    }

def calculate_bleu_scores(prediction: str, reference: str) -> Dict[str, float]:
    """è®¡ç®—BLEUåˆ†æ•°"""
    try:
        pred_tokens = nltk.word_tokenize(prediction.lower())
        ref_tokens = [nltk.word_tokenize(reference.lower())]
        
        weights_list = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]
        smooth = SmoothingFunction().method1
        
        scores = {}
        for n, weights in enumerate(weights_list, start=1):
            score = sentence_bleu(ref_tokens, pred_tokens, weights=weights, smoothing_function=smooth)
            scores[f"bleu{n}"] = score
        
        return scores
    except:
        return {"bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0}

def calculate_sentence_similarity(prediction: str, reference: str) -> float:
    """è®¡ç®—å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦"""
    if sentence_model is None:
        return 0.0
    try:
        embedding1 = sentence_model.encode([prediction], convert_to_tensor=True)
        embedding2 = sentence_model.encode([reference], convert_to_tensor=True)
        similarity = pytorch_cos_sim(embedding1, embedding2).item()
        return float(similarity)
    except:
        return 0.0

def calculate_comprehensive_metrics(prediction: str, reference: str) -> Dict[str, float]:
    """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
    if not prediction or not reference:
        return {
            "rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0,
            "bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0,
            "semantic_similarity": 0.0, "exact_match": 0.0, "f1": 0.0
        }
    
    prediction = str(prediction).strip()
    reference = str(reference).strip()
    
    # ROUGEåˆ†æ•°
    rouge_scores = calculate_rouge_scores(prediction, reference)
    
    # BLEUåˆ†æ•°  
    bleu_scores = calculate_bleu_scores(prediction, reference)
    
    # è¯­ä¹‰ç›¸ä¼¼åº¦
    semantic_sim = calculate_sentence_similarity(prediction, reference)
    
    # ç²¾ç¡®åŒ¹é…
    exact_match = float(prediction.lower() == reference.lower())
    
    # F1åˆ†æ•°ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰
    pred_words = set(prediction.lower().split())
    ref_words = set(reference.lower().split())
    common = pred_words & ref_words
    
    if len(pred_words) == 0 or len(ref_words) == 0:
        f1 = 0.0
    else:
        precision = len(common) / len(pred_words)
        recall = len(common) / len(ref_words)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        **rouge_scores,
        **bleu_scores,
        "semantic_similarity": semantic_sim,
        "exact_match": exact_match,
        "f1": f1
    }

# ===================== LLM Judgeè¯„ä¼° =====================
ACCURACY_PROMPT = """
ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°ä¸€ä¸ªå¿ƒç†å’¨è¯¢é—®é¢˜çš„å›ç­”è´¨é‡ã€‚ä½ å°†æ”¶åˆ°ï¼š
1. ä¸€ä¸ªå…³äºæ‚£è€…çš„é—®é¢˜
2. æ ‡å‡†ç­”æ¡ˆï¼ˆåŸºäºæ‚£è€…çš„çœŸå®ä¿¡æ¯ï¼‰
3. æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼š
- å‡†ç¡®æ€§ï¼šæ˜¯å¦åŒ…å«äº†æ ‡å‡†ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯
- å®Œæ•´æ€§ï¼šæ˜¯å¦æ¶µç›–äº†é‡è¦æ–¹é¢
- ç›¸å…³æ€§ï¼šæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜
- ä¸“ä¸šæ€§ï¼šæ˜¯å¦ç¬¦åˆå¿ƒç†å’¨è¯¢çš„ä¸“ä¸šæ ‡å‡†

é—®é¢˜: {question}
æ ‡å‡†ç­”æ¡ˆ: {gold_answer}
ç”Ÿæˆç­”æ¡ˆ: {generated_answer}

è¯·ç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰å¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚

è¿”å›JSONæ ¼å¼ï¼š{{"score": åˆ†æ•°, "reasoning": "è¯„åˆ†ç†ç”±"}}
"""

def evaluate_with_llm_judge(question: str, gold_answer: str, generated_answer: str) -> Dict:
    """ä½¿ç”¨LLM Judgeè¯„ä¼°å›ç­”è´¨é‡"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": ACCURACY_PROMPT.format(
                    question=question,
                    gold_answer=gold_answer,
                    generated_answer=generated_answer
                )
            }],
            response_format={"type": "json_object"},
            temperature=0.0,
            max_tokens=3000
        )
        
        result = json.loads(response.choices[0].message.content)
        return {
            "llm_score": result.get("score", 0) / 100.0,  # æ ‡å‡†åŒ–åˆ°0-1
            "reasoning": result.get("reasoning", "")
        }
    except Exception as e:
        return {"llm_score": 0.0, "reasoning": f"è¯„ä¼°å¤±è´¥: {e}"}

# ===================== æ•°æ®å¤„ç† =====================
class PatientSession:
    """æ‚£è€…ä¼šè¯æ•°æ®ç±»"""
    def __init__(self, session_data):
        self.dialog_id = session_data.get('dialog_id', '')
        self.patient_id = f"Patient_{self.dialog_id}"
        self.theme = session_data.get('theme', '')
        self.topic = session_data.get('topic', '')
        self.stage = session_data.get('stage', '')
        self.background = session_data.get('background', '')
        self.summary = session_data.get('summary', '')
        self.dialog = session_data.get('dialog', [])
        
        # æå–æƒ…ç»ªå’Œç­–ç•¥
        self.emotions = []
        self.strategies = []
        self.patient_statements = []
        
        for turn in self.dialog:
            if turn.get('emotional label'):
                self.emotions.extend(turn['emotional label'])
            if turn.get('strategy'):
                self.strategies.extend(turn['strategy'])
            if turn.get('speaker') == 'Seeker':
                self.patient_statements.append(turn.get('content', ''))
    
    def get_ground_truth_answers(self) -> Dict[str, str]:
        """ç”Ÿæˆæ ‡å‡†ç­”æ¡ˆ"""
        return {
            "main_emotions": ", ".join(set(self.emotions)) if self.emotions else "æœªè®°å½•æ˜æ˜¾æƒ…ç»ª",
            "core_issues": self.patient_statements[0][:100] + "..." if self.patient_statements else "æœªæ˜ç¡®æåŠ",
            "therapy_strategies": ", ".join(set(self.strategies)) if self.strategies else "æœªä½¿ç”¨ç‰¹å®šç­–ç•¥",
            "session_summary": self.summary[:150] + "..." if self.summary else "æ— ä¼šè¯æ€»ç»“"
        }

def split_patient_dialog(session: PatientSession, split_ratio: float = 0.7):
    """å°†æ‚£è€…å¯¹è¯åˆ†å‰²ä¸ºè®°å¿†å­˜å‚¨éƒ¨åˆ†å’Œæµ‹è¯•éƒ¨åˆ†"""
    total_turns = len(session.dialog)
    split_point = int(total_turns * split_ratio)
    
    memory_turns = session.dialog[:split_point]
    test_turns = session.dialog[split_point:]
    
    return memory_turns, test_turns

# ===================== åŸºçº¿æ¨¡å‹ =====================
class BaselineModel:
    """åŸºçº¿æ¨¡å‹ï¼ˆæ— è®°å¿†ï¼‰"""
    def __init__(self):
        self.total_calls = 0
        self.total_tokens = 0
    
    def answer_question(self, session: PatientSession, question: str, context_turns: List = None) -> str:
        """åŸºäºå½“å‰å¯¹è¯å›ç­”é—®é¢˜"""
        # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä»…ä½¿ç”¨æä¾›çš„å¯¹è¯è½®æ¬¡ï¼‰
        context = f"æ‚£è€…ä¸»é¢˜ï¼š{session.topic}\n"
        if context_turns:
            context += "å¯¹è¯å†…å®¹ï¼š\n"
            for turn in context_turns[-5:]:  # åªä½¿ç”¨æœ€è¿‘5è½®
                speaker = "æ‚£è€…" if turn.get('speaker') == 'Seeker' else "æ²»ç–—å¸ˆ"
                context += f"{speaker}: {turn.get('content', '')}\n"
        
        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

æ³¨æ„ï¼šåªåŸºäºæä¾›çš„ä¿¡æ¯å›ç­”ï¼Œå¦‚æœä¿¡æ¯ä¸è¶³è¯·æ˜ç¡®è¯´æ˜ã€‚"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å¿ƒç†å’¨è¯¢åˆ†æåŠ©æ‰‹ï¼Œåªèƒ½åŸºäºå½“å‰æä¾›çš„ä¿¡æ¯å›ç­”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            self.total_calls += 1
            self.total_tokens += response.usage.total_tokens
            
            return response.choices[0].message.content
        except Exception as e:
            return f"å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{e}"

# ===================== Mem0å¢å¼ºæ¨¡å‹ =====================
class Mem0Model:
    """Mem0å¢å¼ºæ¨¡å‹ï¼ˆæœ‰è®°å¿†ï¼‰"""
    def __init__(self):
        self.total_calls = 0
        self.total_tokens = 0
        self.memory_stored = {}
    
    def store_memory(self, session: PatientSession, memory_turns: List):
        """å­˜å‚¨æ‚£è€…è®°å¿† - æ— é™åˆ¶ç‰ˆæœ¬ï¼Œå­˜å‚¨æ‰€æœ‰å¯¹è¯"""
        patient_id = session.patient_id
        
        # å­˜å‚¨ä¼šè¯æ¦‚å†µ
        session_info = f"""
æ‚£è€…ä¸»é¢˜ï¼š{session.topic}
æ²»ç–—é˜¶æ®µï¼š{session.stage}
èƒŒæ™¯ä¿¡æ¯ï¼š{session.background}
ä¼šè¯æ‘˜è¦ï¼š{session.summary}
æ‚£è€…ä¸»è¦æƒ…ç»ªï¼š{', '.join(set(session.emotions)) if session.emotions else 'æœªæ ‡æ³¨'}
ä½¿ç”¨çš„æ²»ç–—ç­–ç•¥ï¼š{', '.join(set(session.strategies)) if session.strategies else 'æœªæ ‡æ³¨'}
"""
        
        try:
            print(f"å¼€å§‹å­˜å‚¨ {patient_id} çš„è®°å¿†...")
            
            # å­˜å‚¨ä¼šè¯æ¦‚å†µ
            memory.add(
                messages=[{"role": "user", "content": session_info}],
                user_id=patient_id,
                metadata={"type": "session_info", "dialog_id": session.dialog_id}
            )
            print(f"âœ… ä¼šè¯æ¦‚å†µå·²å­˜å‚¨")
            time.sleep(0.5)
            
            # å­˜å‚¨æ‰€æœ‰å¯¹è¯è½®æ¬¡ - æ— ä»»ä½•é™åˆ¶
            total_turns = len(memory_turns)
            stored_count = 0
            
            for i, turn in enumerate(memory_turns):
                speaker = turn.get('speaker', '')
                content = turn.get('content', '').strip()
                turn_id = turn.get('id', f"turn_{i}")
                
                # å­˜å‚¨æ‰€æœ‰è½®æ¬¡ï¼ˆæ‚£è€…å’Œæ²»ç–—å¸ˆçš„è¯éƒ½å­˜å‚¨ï¼‰
                if content:  # åªè¦æœ‰å†…å®¹å°±å­˜å‚¨ï¼Œä¸é™åˆ¶é•¿åº¦
                    # ç¡®å®šè§’è‰²
                    if speaker == 'Seeker':
                        role_label = "æ‚£è€…"
                        content_type = "patient_statement"
                    else:
                        role_label = "æ²»ç–—å¸ˆ" 
                        content_type = "therapist_statement"
                    
                    # æ„å»ºå­˜å‚¨å†…å®¹
                    memory_content = f"{role_label}ï¼ˆç¬¬{i+1}è½®ï¼‰ï¼š{content}"
                    
                    # æ·»åŠ æƒ…ç»ªå’Œç­–ç•¥æ ‡ç­¾
                    emotions = turn.get('emotional label', [])
                    strategies = turn.get('strategy', [])
                    
                    if emotions:
                        memory_content += f"\næƒ…ç»ªæ ‡ç­¾ï¼š{', '.join(emotions)}"
                    if strategies:
                        memory_content += f"\næ²»ç–—ç­–ç•¥ï¼š{', '.join(strategies)}"
                    
                    # å­˜å‚¨åˆ°Mem0
                    try:
                        memory.add(
                            messages=[{"role": "user", "content": memory_content}],
                            user_id=patient_id,
                            metadata={
                                "type": content_type,
                                "turn_index": i,
                                "turn_id": turn_id,
                                "speaker": speaker,
                                "emotions": emotions,
                                "strategies": strategies,
                                "dialog_id": session.dialog_id
                            }
                        )
                        stored_count += 1
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if stored_count % 5 == 0 or stored_count == total_turns:
                            print(f"ğŸ“ å·²å­˜å‚¨ {stored_count}/{total_turns} è½®å¯¹è¯...")
                        
                        # é€‚å½“å»¶è¿Ÿé¿å…APIé™åˆ¶
                        time.sleep(0.3)
                        
                    except Exception as e:
                        print(f"âš ï¸ å­˜å‚¨ç¬¬{i+1}è½®å¤±è´¥ï¼š{e}")
                        # é‡åˆ°é”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                        time.sleep(2)
                        continue
            
            print(f"âœ… {patient_id} è®°å¿†å­˜å‚¨å®Œæˆï¼æ€»å…±å­˜å‚¨äº† {stored_count}/{total_turns} è½®å¯¹è¯")
            self.memory_stored[patient_id] = True
            
            # éªŒè¯å­˜å‚¨ç»“æœ
            self.verify_memory_storage(session)
            
        except Exception as e:
            print(f"âŒ å­˜å‚¨è®°å¿†å¤±è´¥ï¼š{e}")
            self.memory_stored[patient_id] = False
    
    def verify_memory_storage(self, session: PatientSession):
        """éªŒè¯è®°å¿†å­˜å‚¨å®Œæ•´æ€§"""
        patient_id = session.patient_id
        
        try:
            # è·å–æ‰€æœ‰è®°å¿†
            all_memories = memory.get_all(user_id=patient_id)
            
            print(f"\nğŸ“Š {patient_id} è®°å¿†å­˜å‚¨éªŒè¯ï¼š")
            print(f"æ€»è®°å¿†æ•°é‡ï¼š{len(all_memories)}")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            session_info = [m for m in all_memories if 'session_info' in str(m.get('metadata', {}))]
            patient_statements = [m for m in all_memories if 'patient_statement' in str(m.get('metadata', {}))]
            therapist_statements = [m for m in all_memories if 'therapist_statement' in str(m.get('metadata', {}))]
            
            print(f"â”œâ”€ ä¼šè¯ä¿¡æ¯ï¼š{len(session_info)}æ¡")
            print(f"â”œâ”€ æ‚£è€…è¡¨è¿°ï¼š{len(patient_statements)}æ¡")
            print(f"â””â”€ æ²»ç–—å¸ˆè¡¨è¿°ï¼š{len(therapist_statements)}æ¡")
            
            # æ£€æŸ¥å®Œæ•´æ€§
            original_turns = len(session.dialog)
            stored_turns = len(patient_statements) + len(therapist_statements)
            
            print(f"åŸå§‹å¯¹è¯è½®æ¬¡ï¼š{original_turns}è½®")
            print(f"å·²å­˜å‚¨è½®æ¬¡ï¼š{stored_turns}è½®")
            
            if stored_turns >= original_turns * 0.9:  # 90%ä»¥ä¸Šè®¤ä¸ºæˆåŠŸ
                print("âœ… è®°å¿†å­˜å‚¨å®Œæ•´")
            else:
                print(f"âš ï¸ è®°å¿†å­˜å‚¨å¯èƒ½ä¸å®Œæ•´ï¼Œå­˜å‚¨ç‡ï¼š{stored_turns/original_turns*100:.1f}%")
                
            return len(all_memories)
            
        except Exception as e:
            print(f"âŒ éªŒè¯è®°å¿†å­˜å‚¨å¤±è´¥ï¼š{e}")
            return 0
    
    def answer_question(self, session: PatientSession, question: str, context_turns: List = None) -> str:
        """åŸºäºå®Œæ•´è®°å¿†å’Œå½“å‰å¯¹è¯å›ç­”é—®é¢˜"""
        patient_id = session.patient_id
        
        # æœç´¢ç›¸å…³è®°å¿† - å¢åŠ æœç´¢èŒƒå›´
        memories = []
        try:
            search_results = memory.search(
                query=question,
                user_id=patient_id,
                limit=10  # å¢åŠ åˆ°10æ¡è®°å¿†
            )
            memories = search_results if search_results else []
            print(f"ğŸ” ä¸ºé—®é¢˜ '{question}' æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
        except Exception as e:
            print(f"æœç´¢è®°å¿†å¤±è´¥ï¼š{e}")
        
        # æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
        context = f"æ‚£è€…åŸºæœ¬ä¿¡æ¯ï¼š{session.topic}\n"
        context += f"æ²»ç–—é˜¶æ®µï¼š{session.stage}\n"
        context += f"ä¸»è¦æƒ…ç»ªï¼š{', '.join(set(session.emotions)) if session.emotions else 'æœªæ˜ç¡®'}\n\n"
        
        if memories:
            context += "ğŸ“š ç›¸å…³å†å²è®°å¿†ï¼š\n"
            for i, mem in enumerate(memories[:8], 1):  # ä½¿ç”¨æ›´å¤šè®°å¿†
                memory_content = mem.get('memory', '')
                # ä¸æˆªæ–­è®°å¿†å†…å®¹ï¼Œä¿æŒå®Œæ•´æ€§
                context += f"{i}. {memory_content}\n"
            context += "\n"
        else:
            context += "ğŸ“š å†å²è®°å¿†ï¼šæš‚æ— ç›¸å…³è®°å¿†\n\n"
        
        if context_turns:
            context += "ğŸ’¬ å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼š\n"
            # ä½¿ç”¨æ›´å¤šä¸Šä¸‹æ–‡è½®æ¬¡
            recent_turns = context_turns[-10:] if len(context_turns) > 10 else context_turns
            for turn in recent_turns:
                speaker = "æ‚£è€…" if turn.get('speaker') == 'Seeker' else "æ²»ç–—å¸ˆ"
                content = turn.get('content', '')
                emotions = turn.get('emotional label', [])
                strategies = turn.get('strategy', [])
                
                context += f"{speaker}: {content}"
                if emotions:
                    context += f" [æƒ…ç»ª: {', '.join(emotions)}]"
                if strategies:
                    context += f" [ç­–ç•¥: {', '.join(strategies)}]"
                context += "\n"
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºå®Œæ•´çš„å†å²è®°å¿†å’Œå½“å‰å¯¹è¯ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

{context}

ğŸ¯ é—®é¢˜ï¼š{question}

ğŸ“ å›ç­”è¦æ±‚ï¼š
1. å……åˆ†åˆ©ç”¨å†å²è®°å¿†ä¸­çš„ä¿¡æ¯
2. ç»“åˆå½“å‰å¯¹è¯çš„ä¸Šä¸‹æ–‡
3. æä¾›ä¸“ä¸šã€å‡†ç¡®ã€å…¨é¢çš„åˆ†æ
4. å¦‚æœè®°å¿†ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å¼•ç”¨
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜

è¯·ç»™å‡ºè¯¦ç»†çš„ä¸“ä¸šåˆ†æï¼š"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å¿ƒç†å’¨è¯¢åˆ†æåŠ©æ‰‹ï¼Œæ‹¥æœ‰æ‚£è€…çš„å®Œæ•´å†å²è®°å¿†ï¼Œèƒ½å¤Ÿè¿›è¡Œæ·±å…¥çš„å¿ƒç†åˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=3000,  # å¢åŠ å›ç­”é•¿åº¦é™åˆ¶
                temperature=0.3
            )
            
            self.total_calls += 1
            self.total_tokens += response.usage.total_tokens
            
            return response.choices[0].message.content
        except Exception as e:
            return f"å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{e}"

# ===================== ä¸“ä¸šè¯„ä¼°å™¨ =====================
class ProfessionalEvaluator:
    """ä¸“ä¸šè¯„ä¼°å™¨"""
    def __init__(self):
        self.results = {
            'baseline': defaultdict(list),
            'mem0': defaultdict(list)
        }
        # æ–°å¢ï¼šå­˜å‚¨è¯¦ç»†é—®ç­”ä¿¡æ¯
        self.detailed_qa_results = []
        
    def evaluate_single_patient(self, session: PatientSession, baseline_model: BaselineModel, mem0_model: Mem0Model):
        """è¯„ä¼°å•ä¸ªæ‚£è€…"""
        print(f"\nè¯„ä¼°æ‚£è€… {session.patient_id}")
        
        # åˆ†å‰²å¯¹è¯
        memory_turns, test_turns = split_patient_dialog(session, 0.7)
        
        if len(test_turns) == 0:
            print(f"è·³è¿‡ {session.patient_id}ï¼šæµ‹è¯•è½®æ¬¡ä¸è¶³")
            return
        
        # ä¸ºMem0å­˜å‚¨è®°å¿†
        mem0_model.store_memory(session, memory_turns)
        time.sleep(1)  # ç­‰å¾…å­˜å‚¨å®Œæˆ
        
        # å‡†å¤‡æµ‹è¯•é—®é¢˜å’Œæ ‡å‡†ç­”æ¡ˆ
        ground_truths = session.get_ground_truth_answers()
        
        test_questions = [
            ("æ‚£è€…çš„ä¸»è¦æƒ…ç»ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ", ground_truths["main_emotions"]),
            ("æ‚£è€…çš„æ ¸å¿ƒå›°æ‰°æœ‰å“ªäº›ï¼Ÿ", ground_truths["core_issues"]),
            ("æ²»ç–—å¸ˆä½¿ç”¨äº†ä»€ä¹ˆç­–ç•¥ï¼Ÿ", ground_truths["therapy_strategies"]),
            ("è¯·æ€»ç»“è¿™æ¬¡ä¼šè¯çš„ä¸»è¦å†…å®¹", ground_truths["session_summary"])
        ]
        
        # å­˜å‚¨å½“å‰æ‚£è€…çš„é—®ç­”ä¿¡æ¯
        patient_qa_info = {
            'patient_id': session.patient_id,
            'patient_topic': session.topic,
            'patient_theme': session.theme,
            'dialog_length': len(session.dialog),
            'qa_details': []
        }
        
        # å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œè¯„ä¼°
        for question, gold_answer in test_questions:
            print(f"\né—®é¢˜ï¼š{question}")
            
            # åŸºçº¿æ¨¡å‹å›ç­”
            baseline_answer = baseline_model.answer_question(session, question, memory_turns)
            print(f"åŸºçº¿æ¨¡å‹ï¼š{baseline_answer[:100]}...")
            
            # Mem0æ¨¡å‹å›ç­”
            mem0_answer = mem0_model.answer_question(session, question, memory_turns)
            print(f"Mem0æ¨¡å‹ï¼š{mem0_answer[:100]}...")
            
            # è®¡ç®—ç»¼åˆæŒ‡æ ‡
            baseline_metrics = calculate_comprehensive_metrics(baseline_answer, gold_answer)
            mem0_metrics = calculate_comprehensive_metrics(mem0_answer, gold_answer)
            
            # LLM Judgeè¯„ä¼°
            baseline_llm = evaluate_with_llm_judge(question, gold_answer, baseline_answer)
            mem0_llm = evaluate_with_llm_judge(question, gold_answer, mem0_answer)
            
            # å­˜å‚¨ç»“æœ
            self.results['baseline']['rouge1_f'].append(baseline_metrics['rouge1_f'])
            self.results['baseline']['bleu1'].append(baseline_metrics['bleu1'])
            self.results['baseline']['semantic_similarity'].append(baseline_metrics['semantic_similarity'])
            self.results['baseline']['llm_score'].append(baseline_llm['llm_score'])
            self.results['baseline']['f1'].append(baseline_metrics['f1'])
            
            self.results['mem0']['rouge1_f'].append(mem0_metrics['rouge1_f'])
            self.results['mem0']['bleu1'].append(mem0_metrics['bleu1'])
            self.results['mem0']['semantic_similarity'].append(mem0_metrics['semantic_similarity'])
            self.results['mem0']['llm_score'].append(mem0_llm['llm_score'])
            self.results['mem0']['f1'].append(mem0_metrics['f1'])
            
            # å­˜å‚¨è¯¦ç»†é—®ç­”ä¿¡æ¯
            qa_detail = {
                'question': question,
                'gold_answer': gold_answer,
                'baseline_answer': baseline_answer,
                'mem0_answer': mem0_answer,
                'baseline_metrics': baseline_metrics,
                'mem0_metrics': mem0_metrics,
                'baseline_llm_judge': baseline_llm,
                'mem0_llm_judge': mem0_llm,
                'improvement_analysis': {
                    'rouge1_improvement': (mem0_metrics['rouge1_f'] - baseline_metrics['rouge1_f']) / baseline_metrics['rouge1_f'] * 100 if baseline_metrics['rouge1_f'] > 0 else 0,
                    'bleu1_improvement': (mem0_metrics['bleu1'] - baseline_metrics['bleu1']) / baseline_metrics['bleu1'] * 100 if baseline_metrics['bleu1'] > 0 else 0,
                    'semantic_improvement': (mem0_metrics['semantic_similarity'] - baseline_metrics['semantic_similarity']) / baseline_metrics['semantic_similarity'] * 100 if baseline_metrics['semantic_similarity'] > 0 else 0,
                    'llm_judge_improvement': (mem0_llm['llm_score'] - baseline_llm['llm_score']) / baseline_llm['llm_score'] * 100 if baseline_llm['llm_score'] > 0 else 0
                }
            }
            
            patient_qa_info['qa_details'].append(qa_detail)
            
            time.sleep(1)
        
        # å°†æ‚£è€…ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
        self.detailed_qa_results.append(patient_qa_info)
    
    def generate_comparison_report(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = f"""
================================================================================
                    ä¸“ä¸šè¯„ä¼°ç‰ˆ Mem0 vs åŸºçº¿æ¨¡å‹ å¯¹æ¯”æŠ¥å‘Š
================================================================================

è¯„ä¼°æŒ‡æ ‡è¯´æ˜ï¼š
- ROUGE-1: å•è¯é‡å åº¦ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
- BLEU-1: ç¿»è¯‘è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰  
- è¯­ä¹‰ç›¸ä¼¼åº¦: å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
- LLM Judge: ä¸“ä¸šè¯„åˆ¤åˆ†æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
- F1åˆ†æ•°: è¯æ±‡é‡å F1ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰

================================================================================
                              è¯¦ç»†å¯¹æ¯”ç»“æœ
================================================================================

æŒ‡æ ‡                    åŸºçº¿æ¨¡å‹      Mem0æ¨¡å‹      æå‡ç‡        æ˜¾è‘—æ€§
--------------------------------------------------------------------------------
"""
        
        metrics = ['rouge1_f', 'bleu1', 'semantic_similarity', 'llm_score', 'f1']
        metric_names = ['ROUGE-1', 'BLEU-1', 'è¯­ä¹‰ç›¸ä¼¼åº¦', 'LLM Judge', 'F1åˆ†æ•°']
        
        for metric, name in zip(metrics, metric_names):
            baseline_scores = self.results['baseline'][metric]
            mem0_scores = self.results['mem0'][metric]
            
            if baseline_scores and mem0_scores:
                baseline_mean = statistics.mean(baseline_scores)
                mem0_mean = statistics.mean(mem0_scores)
                improvement = (mem0_mean - baseline_mean) / baseline_mean * 100 if baseline_mean > 0 else 0
                
                # ç®€å•çš„æ˜¾è‘—æ€§æ£€éªŒï¼ˆæ¯”è¾ƒå‡å€¼ï¼‰
                significance = "æ˜¾è‘—" if abs(improvement) > 10 else "ä¸æ˜¾è‘—"
                
                report += f"{name:<15} {baseline_mean:.4f}      {mem0_mean:.4f}      {improvement:+.1f}%       {significance}\n"
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        baseline_overall = []
        mem0_overall = []
        
        for metric in metrics:
            if self.results['baseline'][metric] and self.results['mem0'][metric]:
                baseline_overall.extend(self.results['baseline'][metric])
                mem0_overall.extend(self.results['mem0'][metric])
        
        if baseline_overall and mem0_overall:
            baseline_avg = statistics.mean(baseline_overall)
            mem0_avg = statistics.mean(mem0_overall)
            overall_improvement = (mem0_avg - baseline_avg) / baseline_avg * 100 if baseline_avg > 0 else 0
            
            report += f"\n{'ç»¼åˆå¾—åˆ†':<15} {baseline_avg:.4f}      {mem0_avg:.4f}      {overall_improvement:+.1f}%       {'æ˜¾è‘—' if abs(overall_improvement) > 5 else 'ä¸æ˜¾è‘—'}\n"
        
        report += f"""
================================================================================
                              ç»“è®ºå’Œå»ºè®®
================================================================================

1. æ€§èƒ½æå‡åˆ†æï¼š
   - å¦‚æœå¤šé¡¹æŒ‡æ ‡æå‡>10%ï¼ŒMem0æ¡†æ¶æ•ˆæœæ˜¾è‘—
   - å¦‚æœæå‡<5%ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–è®°å¿†å­˜å‚¨ç­–ç•¥
   - LLM Judgeåˆ†æ•°æœ€èƒ½åæ˜ å®é™…åº”ç”¨ä»·å€¼

2. æ”¹è¿›å»ºè®®ï¼š
   - ä¼˜åŒ–è®°å¿†å­˜å‚¨å†…å®¹çš„é€‰æ‹©
   - æ”¹è¿›è®°å¿†æ£€ç´¢çš„ç›¸å…³æ€§
   - è€ƒè™‘å¼•å…¥æ›´å¤šé¢†åŸŸçŸ¥è¯†

3. å±€é™æ€§è¯´æ˜ï¼š
   - æµ‹è¯•æ•°æ®é‡å¯èƒ½ä¸è¶³
   - è¯„ä¼°æ ‡å‡†ç­”æ¡ˆè´¨é‡æœ‰å¾…æé«˜
   - éœ€è¦æ›´å¤šçœŸå®åœºæ™¯éªŒè¯

================================================================================
"""
        
        return report
    
    def generate_detailed_qa_document(self) -> str:
        """ç”Ÿæˆè¯¦ç»†é—®ç­”è¿‡ç¨‹æ–‡æ¡£"""
        doc = f"""# Mem0 vs åŸºçº¿æ¨¡å‹ - è¯¦ç»†é—®ç­”è¿‡ç¨‹å¯¹æ¯”æŠ¥å‘Šï¼ˆå®Œæ•´è®°å¿†ç‰ˆï¼‰

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**å®éªŒè¯´æ˜**: ä½¿ç”¨ä¸“ä¸šNLPè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”Mem0å¢å¼ºæ¨¡å‹ï¼ˆå®Œæ•´è®°å¿†å­˜å‚¨ï¼‰ä¸åŸºçº¿æ¨¡å‹çš„å¿ƒç†å’¨è¯¢é—®ç­”æ•ˆæœ
**è®°å¿†ç­–ç•¥**: æ— é™åˆ¶å­˜å‚¨ - æ‚£è€…å’Œæ²»ç–—å¸ˆçš„æ‰€æœ‰å¯¹è¯è½®æ¬¡

---

"""
        
        for patient_info in self.detailed_qa_results:
            doc += f"""## ğŸ§‘â€âš•ï¸ {patient_info['patient_id']} è¯„ä¼°ç»“æœ

**æ‚£è€…ä¿¡æ¯**:
- ä¸»é¢˜: {patient_info['patient_topic']}
- ä¸»é¢˜åˆ†ç±»: {patient_info['patient_theme']}
- å¯¹è¯è½®æ¬¡: {patient_info['dialog_length']}è½®ï¼ˆå®Œæ•´å­˜å‚¨ï¼‰

---

"""
            
            for i, qa_detail in enumerate(patient_info['qa_details'], 1):
                doc += f"""### Q{i}: {qa_detail['question']}

**ğŸ“š æ ‡å‡†ç­”æ¡ˆ (Ground Truth):**
> {qa_detail['gold_answer']}

**ğŸ”µ åŸºçº¿æ¨¡å‹å›ç­”:**
> {qa_detail['baseline_answer']}

**ğŸŸ¢ Mem0æ¨¡å‹å›ç­”ï¼ˆåŸºäºå®Œæ•´è®°å¿†ï¼‰:**
> {qa_detail['mem0_answer']}

**ğŸ“Š è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”:**

| æŒ‡æ ‡ | åŸºçº¿æ¨¡å‹ | Mem0æ¨¡å‹ï¼ˆå®Œæ•´è®°å¿†ï¼‰ | æå‡ç‡ |
|------|----------|----------|--------|
| ROUGE-1 | {qa_detail['baseline_metrics']['rouge1_f']:.4f} | {qa_detail['mem0_metrics']['rouge1_f']:.4f} | {qa_detail['improvement_analysis']['rouge1_improvement']:+.1f}% |
| BLEU-1 | {qa_detail['baseline_metrics']['bleu1']:.4f} | {qa_detail['mem0_metrics']['bleu1']:.4f} | {qa_detail['improvement_analysis']['bleu1_improvement']:+.1f}% |
| è¯­ä¹‰ç›¸ä¼¼åº¦ | {qa_detail['baseline_metrics']['semantic_similarity']:.4f} | {qa_detail['mem0_metrics']['semantic_similarity']:.4f} | {qa_detail['improvement_analysis']['semantic_improvement']:+.1f}% |
| LLM Judge | {qa_detail['baseline_llm_judge']['llm_score']:.4f} | {qa_detail['mem0_llm_judge']['llm_score']:.4f} | {qa_detail['improvement_analysis']['llm_judge_improvement']:+.1f}% |
| F1åˆ†æ•° | {qa_detail['baseline_metrics']['f1']:.4f} | {qa_detail['mem0_metrics']['f1']:.4f} | {(qa_detail['mem0_metrics']['f1'] - qa_detail['baseline_metrics']['f1']) / qa_detail['baseline_metrics']['f1'] * 100 if qa_detail['baseline_metrics']['f1'] > 0 else 0:+.1f}% |

**ğŸ” LLM Judge è¯„ä¼°ç†ç”±:**

*åŸºçº¿æ¨¡å‹:* {qa_detail['baseline_llm_judge']['reasoning']}

*Mem0æ¨¡å‹ï¼ˆå®Œæ•´è®°å¿†ï¼‰:* {qa_detail['mem0_llm_judge']['reasoning']}

**ğŸ’¡ å®Œæ•´è®°å¿†ä¼˜åŠ¿åˆ†æ:**
"""
                
                # åˆ†æå®Œæ•´è®°å¿†çš„ä¼˜åŠ¿
                improvements = qa_detail['improvement_analysis']
                significant_improvements = [(k, v) for k, v in improvements.items() if v > 10]
                
                if significant_improvements:
                    doc += "- **æ˜¾è‘—æå‡çš„æŒ‡æ ‡**: "
                    doc += ", ".join([f"{k.replace('_improvement', '')}: +{v:.1f}%" for k, v in significant_improvements])
                    doc += "\n"
                
                # åˆ†æè®°å¿†ä½¿ç”¨æƒ…å†µ
                if "å†å²è®°å¿†" in qa_detail['mem0_answer'] or "æ ¹æ®è®°å¿†" in qa_detail['mem0_answer']:
                    doc += "- **è®°å¿†æ•´åˆæˆåŠŸ**: Mem0æ¨¡å‹æˆåŠŸæ•´åˆäº†å®Œæ•´çš„å†å²è®°å¿†ä¿¡æ¯\n"
                
                if "æ‚£è€…è¡¨è¿°" in qa_detail['mem0_answer'] or "æ²»ç–—å¸ˆ" in qa_detail['mem0_answer']:
                    doc += "- **å®Œæ•´å¯¹è¯åˆ©ç”¨**: å……åˆ†åˆ©ç”¨äº†æ‚£è€…å’Œæ²»ç–—å¸ˆçš„å®Œæ•´å¯¹è¯å†å²\n"
                
                # åˆ†æå›ç­”è´¨é‡å·®å¼‚
                baseline_len = len(qa_detail['baseline_answer'])
                mem0_len = len(qa_detail['mem0_answer'])
                
                if mem0_len > baseline_len * 1.2:
                    doc += f"- **å›ç­”æ›´è¯¦ç»†**: Mem0å›ç­”æ¯”åŸºçº¿æ¨¡å‹è¯¦ç»†{(mem0_len/baseline_len-1)*100:.1f}%\n"
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æƒ…ç»ªå’Œç­–ç•¥ä¿¡æ¯
                if "æƒ…ç»ª" in qa_detail['mem0_answer'] or "ç­–ç•¥" in qa_detail['mem0_answer']:
                    doc += "- **ä¸“ä¸šä¿¡æ¯æ•´åˆ**: æˆåŠŸæ•´åˆäº†æƒ…ç»ªæ ‡ç­¾å’Œæ²»ç–—ç­–ç•¥ä¿¡æ¯\n"
                
                doc += "\n---\n\n"
        
        # æ·»åŠ å®Œæ•´è®°å¿†å®éªŒçš„æ€»ä½“ç»Ÿè®¡
        doc += f"""## ğŸ“ˆ å®Œæ•´è®°å¿†å®éªŒæ€»ä½“ç»Ÿè®¡

**å®éªŒç‰¹è‰²**: 
- âœ… æ— å­˜å‚¨è½®æ¬¡é™åˆ¶ - å­˜å‚¨æ‚£è€…çš„æ‰€æœ‰å¯¹è¯
- âœ… æ— å†…å®¹é•¿åº¦é™åˆ¶ - å­˜å‚¨å®Œæ•´çš„è¡¨è¿°å†…å®¹  
- âœ… åŒè§’è‰²å­˜å‚¨ - åŒæ—¶å­˜å‚¨æ‚£è€…å’Œæ²»ç–—å¸ˆçš„å‘è¨€
- âœ… ä¸°å¯Œå…ƒæ•°æ® - åŒ…å«æƒ…ç»ªæ ‡ç­¾ã€æ²»ç–—ç­–ç•¥ç­‰ä¿¡æ¯

**æµ‹è¯•è§„æ¨¡**:
- æµ‹è¯•æ‚£è€…æ€»æ•°: {len(self.detailed_qa_results)}
- é—®ç­”å¯¹æ€»æ•°: {sum(len(p['qa_details']) for p in self.detailed_qa_results)}
- å¹³å‡å¯¹è¯è½®æ¬¡: {sum(p['dialog_length'] for p in self.detailed_qa_results) / len(self.detailed_qa_results):.1f}è½®

### å®Œæ•´è®°å¿†çš„æ•ˆæœåˆ†æ

"""
        
        # è®¡ç®—å¹³å‡æå‡ç‡
        all_improvements = defaultdict(list)
        memory_utilization_count = 0
        total_qa_pairs = 0
        
        for patient_info in self.detailed_qa_results:
            for qa_detail in patient_info['qa_details']:
                total_qa_pairs += 1
                for metric, improvement in qa_detail['improvement_analysis'].items():
                    all_improvements[metric].append(improvement)
                
                # ç»Ÿè®¡è®°å¿†åˆ©ç”¨æƒ…å†µ
                if "å†å²è®°å¿†" in qa_detail['mem0_answer'] or "æ ¹æ®è®°å¿†" in qa_detail['mem0_answer']:
                    memory_utilization_count += 1
        
        memory_utilization_rate = (memory_utilization_count / total_qa_pairs) * 100
        
        doc += f"**è®°å¿†åˆ©ç”¨ç‡**: {memory_utilization_rate:.1f}% ({memory_utilization_count}/{total_qa_pairs} ä¸ªå›ç­”æˆåŠŸåˆ©ç”¨äº†å†å²è®°å¿†)\n\n"
        
        doc += "**å„æŒ‡æ ‡å¹³å‡æå‡ç‡**:\n\n"
        for metric, improvements in all_improvements.items():
            avg_improvement = statistics.mean(improvements)
            metric_name = metric.replace('_improvement', '').replace('_', ' ').title()
            doc += f"- **{metric_name}**: {avg_improvement:+.1f}%\n"
        
        # è®¡ç®—æ•´ä½“æ•ˆæœ
        overall_avg = statistics.mean([statistics.mean(improvements) for improvements in all_improvements.values()])
        
        doc += f"\n**æ•´ä½“å¹³å‡æå‡**: {overall_avg:+.1f}%\n\n"
        
        doc += f"""### å®Œæ•´è®°å¿†å®éªŒç»“è®º

"""
        
        # æ ¹æ®ç»“æœç”Ÿæˆç»“è®º
        if overall_avg > 15:
            doc += "ğŸ‰ **å®Œæ•´è®°å¿†ç­–ç•¥æ•ˆæœæ˜¾è‘—**ï¼Mem0åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šéƒ½æœ‰å¤§å¹…æå‡ï¼Œè¯æ˜äº†æ— é™åˆ¶è®°å¿†å­˜å‚¨çš„ä¼˜åŠ¿ã€‚\n\n"
        elif overall_avg > 5:
            doc += "âœ… **å®Œæ•´è®°å¿†ç­–ç•¥æœ‰æ•ˆ**ã€‚Mem0æ¨¡å‹è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå®Œæ•´è®°å¿†å­˜å‚¨å¸¦æ¥äº†æ˜æ˜¾æ”¹è¿›ã€‚\n\n"
        else:
            doc += "ğŸ¤” **å®Œæ•´è®°å¿†æ•ˆæœæœ‰é™**ã€‚å°½ç®¡å­˜å‚¨äº†å®Œæ•´è®°å¿†ï¼Œæå‡æ•ˆæœä»ç„¶ä¸å¤Ÿæ˜¾è‘—ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–è®°å¿†æ£€ç´¢ç­–ç•¥ã€‚\n\n"
        
        doc += f"""### å®Œæ•´è®°å¿†çš„ä¼˜åŠ¿æ€»ç»“

1. **ä¿¡æ¯å®Œæ•´æ€§**: å­˜å‚¨äº†æ‚£è€…å’Œæ²»ç–—å¸ˆçš„æ‰€æœ‰å¯¹è¯ï¼Œä¿æŒäº†å®Œæ•´çš„æ²»ç–—ä¸Šä¸‹æ–‡
2. **ç»†èŠ‚ä¿ç•™**: ä¸é™åˆ¶å†…å®¹é•¿åº¦ï¼Œä¿ç•™äº†å®Œæ•´çš„è¡¨è¿°å’Œç»†èŠ‚ä¿¡æ¯
3. **å…ƒæ•°æ®ä¸°å¯Œ**: åŒ…å«æƒ…ç»ªæ ‡ç­¾ã€æ²»ç–—ç­–ç•¥ç­‰ä¸“ä¸šå¿ƒç†å’¨è¯¢ä¿¡æ¯
4. **æ—¶é—´è¿ç»­æ€§**: å®Œæ•´çš„æ—¶é—´çº¿è®©AIèƒ½å¤Ÿç†è§£æ²»ç–—çš„å‘å±•è¿‡ç¨‹

### æ”¹è¿›å»ºè®®

1. **ä¼˜åŒ–æ£€ç´¢ç®—æ³•**: åœ¨å®Œæ•´è®°å¿†çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›è¯­ä¹‰æ£€ç´¢çš„ç›¸å…³æ€§
2. **è®°å¿†é‡è¦æ€§æƒé‡**: ä¸ºä¸åŒç±»å‹çš„è®°å¿†èµ‹äºˆä¸åŒæƒé‡
3. **åŠ¨æ€è®°å¿†é€‰æ‹©**: æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„è®°å¿†ç‰‡æ®µ
4. **è·¨ä¼šè¯è®°å¿†**: å¦‚æœæœ‰å¤šä¸ªä¼šè¯ï¼Œè€ƒè™‘è·¨ä¼šè¯çš„è®°å¿†æ•´åˆ

---

*æœ¬æŠ¥å‘Šå±•ç¤ºäº†å®Œæ•´è®°å¿†å­˜å‚¨ç­–ç•¥çš„æ•ˆæœã€‚é€šè¿‡å­˜å‚¨æ‚£è€…çš„æ‰€æœ‰å¯¹è¯è½®æ¬¡ï¼ŒMem0èƒ½å¤Ÿæä¾›æ›´å…¨é¢ã€æ›´ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åˆ†æã€‚*
"""
        
        return doc

# ===================== ä¸»å‡½æ•° =====================
def main():
    print("\n" + "="*80)
    print("        ä¸“ä¸šè¯„ä¼°ç‰ˆ Mem0 å¿ƒç†å’¨è¯¢å®éªŒ")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    file_path = r"D:\0805-1\Psy-Insight-main\data\cn_data_version7.json"
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"åŠ è½½äº† {len(data)} ä¸ªæ‚£è€…ä¼šè¯")
    
    # åˆå§‹åŒ–æ¨¡å‹
    baseline_model = BaselineModel()
    mem0_model = Mem0Model()
    evaluator = ProfessionalEvaluator()
    
    # é€‰æ‹©æœ‰è¶³å¤Ÿå¯¹è¯è½®æ¬¡çš„æ‚£è€…ï¼ˆè‡³å°‘10è½®ï¼‰
    suitable_patients = []
    for session_data in data[:50]:  # åªæµ‹è¯•å‰50ä¸ª
        session = PatientSession(session_data)
        if len(session.dialog) >= 10:
            suitable_patients.append(session)
    
    print(f"æ‰¾åˆ° {len(suitable_patients)} ä¸ªé€‚åˆæµ‹è¯•çš„æ‚£è€…")
    
    # è¯„ä¼°æ¯ä¸ªæ‚£è€…
    for i, session in enumerate(suitable_patients[:10]):  # åªè¯„ä¼°å‰10ä¸ªæ‚£è€…
        print(f"\nè¿›åº¦ï¼š{i+1}/{min(10, len(suitable_patients))}")
        try:
            evaluator.evaluate_single_patient(session, baseline_model, mem0_model)
        except Exception as e:
            print(f"è¯„ä¼° {session.patient_id} æ—¶å‡ºé”™ï¼š{e}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_comparison_report()
    print(report)
    
    # ç”Ÿæˆè¯¦ç»†é—®ç­”æ–‡æ¡£
    detailed_qa_doc = evaluator.generate_detailed_qa_document()
    
    # ä¿å­˜ç»“æœ
    with open('professional_evaluation_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    with open('professional_evaluation_data.json', 'w', encoding='utf-8') as f:
        json.dump(dict(evaluator.results), f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜è¯¦ç»†é—®ç­”è¿‡ç¨‹æ–‡æ¡£
    with open('detailed_qa_comparison_report.md', 'w', encoding='utf-8') as f:
        f.write(detailed_qa_doc)
    
    # åŒæ—¶ä¿å­˜ä¸ºtxtæ ¼å¼
    with open('detailed_qa_comparison_report.txt', 'w', encoding='utf-8') as f:
        f.write(detailed_qa_doc)
    
    print("\n" + "="*80)
    print("ğŸ“ è¯„ä¼°å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬ï¼š")
    print("   - professional_evaluation_report.txt (å¯¹æ¯”æŠ¥å‘Š)")
    print("   - professional_evaluation_data.json (åŸå§‹æ•°æ®)")  
    print("   - detailed_qa_comparison_report.md (è¯¦ç»†é—®ç­”è¿‡ç¨‹)")
    print("   - detailed_qa_comparison_report.txt (è¯¦ç»†é—®ç­”è¿‡ç¨‹)")
    print("="*80)

if __name__ == "__main__":
    main()